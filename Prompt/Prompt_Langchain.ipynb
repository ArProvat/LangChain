{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CQhpEQDNi34",
        "outputId": "eb701728-b711-4a1f-f3e5-01dc7eab3abe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from Langchain) (0.3.68)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from Langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from Langchain) (0.4.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from Langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from Langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from Langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from Langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->Langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->Langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->Langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->Langchain) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->Langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->Langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->Langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->Langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->Langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->Langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->Langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->Langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->Langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->Langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->Langchain) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->Langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->Langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->Langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->Langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->Langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->Langchain) (1.3.1)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.3.68)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.2 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.33.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (1.1.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (0.4.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (2.11.7)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain-huggingface) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain-huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain-huggingface) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain-huggingface) (2025.6.15)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (1.3.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.6.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install Langchain\n",
        "!pip install langchain-huggingface\n",
        "!pip install huggingface_hub\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MTWQ4YO0NxF1",
        "outputId": "5678d2ff-202c-45fa-ecca-50858f886a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AIzaSyCK0P_IRUruAIR3MShNXWy8Kq08MwZdrnQ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from re import template\n",
        "from langchain_huggingface import HuggingFaceEndpoint,ChatHuggingFace\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "import os\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "llm = HuggingFaceEndpoint(repo_id='deepseek-ai/DeepSeek-R1', temperature=0.1)\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "template = PromptTemplate(\n",
        "    template= '''Please summarize the research paper titled \"{paper_input}\" with the following specifications:\n",
        "    explanation style:{explanation_style}\n",
        "    summary length:{summary_length}\n",
        "    ''',\n",
        "    input_variables=['paper_input','explanation_style','summary_length'],\n",
        "    validate_template=True #validate the prompt(check number of input variable are equal or not)\n",
        "    )\n",
        "prompt = template.invoke({'paper_input':'Attention all you need','explanation_style':\"Beginner-Friendly\",'summary_length':\"5 line\"})\n",
        "result = model.invoke(prompt)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7monf7zaXND",
        "outputId": "1406621d-ccd6-4472-dc8e-1f0ac1b37367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='<think>\\nOkay, so the user wants a summary of the \"Attention is All You Need\" paper, right? Let me start by recalling what that paper is about. Oh right, it introduced the Transformer model. Now, they specified the explanation style as beginner-friendly and a 5-line summary.\\n\\nFirst, I need to make sure I don\\'t use any jargon. Maybe start with saying it\\'s a 2017 paper by Google. Then explain the main innovation was the Transformer using attention mechanisms instead of RNNs or CNNs. Need to highlight why that\\'s important—like handling long data better and being faster. Mention self-attention for context. Also, mention the impact it had on models like BERT and GPT. Keep each point concise to fit in five lines without missing key points. Let me check each line for clarity. Maybe start with the basics, then what it does, how it\\'s different, key components, and applications. Does that cover everything without being too technical?\\n</think>\\n\\n**Summary of \"Attention Is All You Need\":**  \\nThis 2017 paper introduced the **Transformer**, a neural network for language tasks like translation. Instead of traditional methods (e.g., RNNs/CNNs), it uses **attention mechanisms** to focus on important words in a sentence, making training faster and better at handling long text. The key innovation is \"self-attention,\" letting words directly compare themselves to others for richer context. Transformers paved the way for ChatGPT, BERT, and modern AI language tools. Simplified: *Focus where it matters, no repeats, results improved!*' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 327, 'prompt_tokens': 36, 'total_tokens': 363}, 'model_name': 'deepseek-ai/DeepSeek-R1', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run--65eb72e4-6dcf-4dbc-b672-1b7d6cb4a073-0' usage_metadata={'input_tokens': 36, 'output_tokens': 327, 'total_tokens': 363}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "65v_vGaCbZXa",
        "outputId": "06e961de-6985-4cc9-dab8-825798a581c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<think>\\nOkay, so the user wants a summary of the \"Attention is All You Need\" paper, right? Let me start by recalling what that paper is about. Oh right, it introduced the Transformer model. Now, they specified the explanation style as beginner-friendly and a 5-line summary.\\n\\nFirst, I need to make sure I don\\'t use any jargon. Maybe start with saying it\\'s a 2017 paper by Google. Then explain the main innovation was the Transformer using attention mechanisms instead of RNNs or CNNs. Need to highlight why that\\'s important—like handling long data better and being faster. Mention self-attention for context. Also, mention the impact it had on models like BERT and GPT. Keep each point concise to fit in five lines without missing key points. Let me check each line for clarity. Maybe start with the basics, then what it does, how it\\'s different, key components, and applications. Does that cover everything without being too technical?\\n</think>\\n\\n**Summary of \"Attention Is All You Need\":**  \\nThis 2017 paper introduced the **Transformer**, a neural network for language tasks like translation. Instead of traditional methods (e.g., RNNs/CNNs), it uses **attention mechanisms** to focus on important words in a sentence, making training faster and better at handling long text. The key innovation is \"self-attention,\" letting words directly compare themselves to others for richer context. Transformers paved the way for ChatGPT, BERT, and modern AI language tools. Simplified: *Focus where it matters, no repeats, results improved!*'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Messages\n",
        "**Messages** are the unit of communication in chat models. They are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation.\n",
        "\n",
        "**SystemMessage**: corresponds to system role\n",
        "\n",
        "**HumanMessage**: corresponds to user role\n",
        "\n",
        "**AIMessage**: corresponds to assistant role"
      ],
      "metadata": {
        "id": "0McnrxD7ctyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint,ChatHuggingFace\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.schema import AIMessage,HumanMessage,SystemMessage\n",
        "import os\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm=HuggingFaceEndpoint(repo_id=repo_id,temperature=0.7)\n",
        "\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "message =[\n",
        "    SystemMessage(content=\"you are a helpful assistant that help to translate english to french\"),\n",
        "    HumanMessage(content=\"Translate the following sentence: I love programming\")\n",
        "]\n",
        "\n",
        "result = model.invoke(message)\n",
        "message.append(AIMessage(result.content))\n",
        "print(message)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hs2LGSKSbbIV",
        "outputId": "662d6224-d4d1-4793-dc72-b09338a96d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! max_length is not default parameter.\n",
            "                    max_length was transferred to model_kwargs.\n",
            "                    Please make sure that max_length is what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content='you are a helpful assistant that help to translate english to french', additional_kwargs={}, response_metadata={}), HumanMessage(content='Translate the following sentence: I love programming', additional_kwargs={}, response_metadata={}), AIMessage(content='Je adore le codage', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "make small **chatbot**\n",
        "using  message"
      ],
      "metadata": {
        "id": "40BHwvk8fSsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint,ChatHuggingFace\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.schema import AIMessage,HumanMessage,SystemMessage\n",
        "import os\n",
        "\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint,ChatHuggingFace\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.schema import AIMessage,HumanMessage,SystemMessage\n",
        "import os\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm=HuggingFaceEndpoint(repo_id=repo_id,temperature=0.7)\n",
        "\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "message =[\n",
        "    SystemMessage(content=\"you are a helpful assistant \"),\n",
        "]\n",
        "\n",
        "while True:\n",
        " input_prompt = input(\"user: \")\n",
        " message.append(HumanMessage(content = input_prompt))\n",
        " if input_prompt == \"quit\":\n",
        "  break\n",
        " result = model.invoke(message)\n",
        " message.append(AIMessage(result.content))\n",
        " print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41fjcAy9fSIr",
        "outputId": "fca0c766-443f-41d3-a3c5-2f2e8b0617ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user: Hi\n",
            "Hello, how can I help you today?\n",
            "\n",
            "user: can you telll me which number is greate between 7,3\n",
            " sure, 7 is greater than 3.\n",
            "\n",
            "user: now multiply  it with 10\n",
            " Let's calculate, 7 * 10 = 70.\n",
            "\n",
            "user: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOW create a chatBot using Dynamic message and message placeholder"
      ],
      "metadata": {
        "id": "SY72LZKbhx-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint,ChatHuggingFace\n",
        "from langchain_core.prompts import ChatPromptTemplate ,MessagesPlaceholder\n",
        "from langchain.schema import AIMessage,HumanMessage,SystemMessage\n",
        "import os\n",
        "\n",
        "llm = HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7)\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "message = ChatPromptTemplate.from_messages([\n",
        "    ('system',\"YOu are a help assistant\"),\n",
        "    MessagesPlaceholder(variable_name='chat_history'),\n",
        "    ('human','{qurey}'),\n",
        "])\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "with open('Chat_history.txt','r') as f:\n",
        "  chat_history = f.read().split('\\n')\n",
        "\n",
        "while True:\n",
        "  User_input =input(\"user: \")\n",
        "  if User_input == \"quit\":\n",
        "    break\n",
        "  prompt= message.invoke({'chat_history':chat_history,'qurey':User_input})\n",
        "  response = model.invoke(prompt)\n",
        "  print(response)\n",
        "  chat_history.append(HumanMessage(content=User_input))\n",
        "  chat_history.append(AIMessage(content=response.content))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCItSAosiABz",
        "outputId": "d7945455-da77-44b3-8437-fd5234072b16"
      },
      "execution_count": 31,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user: where is my refund\n",
            "content='AIMessage(content=\"Your refund for order #12345 is currently being processed. It will be completed in 3-5 business days.\")' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 160, 'total_tokens': 199}, 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None} id='run--07708a13-6ab2-4afe-9fa5-fff439b7f7a8-0' usage_metadata={'input_tokens': 160, 'output_tokens': 39, 'total_tokens': 199}\n",
            "user: quit\n"
          ]
        }
      ]
    }
  ]
}